{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419b702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\Pop\\Documents\\GitHub\\utcc_independent_study\\training\\..\\data\\300_data_pop.xlsx\n",
      "data_size: 318\n",
      "variable: data train_data test_data\n"
     ]
    }
   ],
   "source": [
    "%run data_read.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b803669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pop\\anaconda3\\envs\\utcc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 1326\n",
      "num_words after: 645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score_norm</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ตัวหูฟังมีรอยแตกทั้ง2ข้าง ตอนนี้ยังไม่มีผลต่อก...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>[2, 3, 4, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ซื้อสินค้าไปดูไม่ค่อยแข็งแรง ใช้ไม่ได้ด้วยค่ะ ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[22, 23, 1, 24, 25, 8, 26, 27, 28, 8, 1, 8, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>สินค้าใช้ไม่ได้ ร้านค้าแจ้งไม่ต้องกดในระบบ พอเ...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>[23, 26, 8, 47, 48, 49, 50, 51, 52, 8, 53, 35,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ของพึ่งได้มาเมื่อวานใช้ตอนเช้าฟังได้แค่ข้างเดี...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[58, 59, 60, 61, 62, 63, 64, 20, 65, 66, 67, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ประสิทธิภาพ: ใช้พูดคุยไม่ได้เสียงเบามาก ,คนฟัง...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>[68, 69, 8, 62, 1, 11, 70, 71, 72, 73, 8, 74, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  score_norm  \\\n",
       "0  ตัวหูฟังมีรอยแตกทั้ง2ข้าง ตอนนี้ยังไม่มีผลต่อก...    0.444444   \n",
       "1  ซื้อสินค้าไปดูไม่ค่อยแข็งแรง ใช้ไม่ได้ด้วยค่ะ ...    1.000000   \n",
       "2  สินค้าใช้ไม่ได้ ร้านค้าแจ้งไม่ต้องกดในระบบ พอเ...    0.888889   \n",
       "3  ของพึ่งได้มาเมื่อวานใช้ตอนเช้าฟังได้แค่ข้างเดี...    0.666667   \n",
       "4  ประสิทธิภาพ: ใช้พูดคุยไม่ได้เสียงเบามาก ,คนฟัง...    0.777778   \n",
       "\n",
       "                                             encoded  \n",
       "0  [2, 3, 4, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 8,...  \n",
       "1  [22, 23, 1, 24, 25, 8, 26, 27, 28, 8, 1, 8, 29...  \n",
       "2  [23, 26, 8, 47, 48, 49, 50, 51, 52, 8, 53, 35,...  \n",
       "3  [58, 59, 60, 61, 62, 63, 64, 20, 65, 66, 67, 5...  \n",
       "4  [68, 69, 8, 62, 1, 11, 70, 71, 72, 73, 8, 74, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def thai_tokenizer(text):\n",
    "    text = normalize(text)\n",
    "    tokens = word_tokenize(text, engine='newmm')\n",
    "    return tokens\n",
    "\n",
    "counts = Counter()\n",
    "for text in list(data['comment']):\n",
    "    counts.update(thai_tokenizer(text))\n",
    "\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))\n",
    "\n",
    "# Creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "    \n",
    "def encode_sentence(text, vocab2index, N=200):\n",
    "    tokenized = thai_tokenizer(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded\n",
    "\n",
    "data['encoded'] = data['comment'].apply(lambda x: np.array(encode_sentence(x, vocab2index)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b025907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitReadabiltyDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx].astype(np.int32)), self.y[idx], self.X[idx][1] \n",
    "\n",
    "X = list(data['encoded'])\n",
    "y = list(data['score_norm'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds = CommonLitReadabiltyDataset(X_train, y_train)\n",
    "valid_ds = CommonLitReadabiltyDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a047c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regr(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        # print('epochs:', i, end=' ')\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss = validation_metrics_regr(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train mse %.4f val rmse %.4f\" % (sum_loss/total, val_loss))\n",
    "\n",
    "def validation_metrics_regr (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.float()\n",
    "        y_hat = model(x, l)\n",
    "        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total\n",
    "\n",
    "batch_size = 16\n",
    "vocab_size = len(words)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 200\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c91b3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_regr(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb8acd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  LSTM_regr(vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69563c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse 0.2794 val rmse 0.5146\n",
      "train mse 0.1055 val rmse 0.2553\n",
      "train mse 0.0771 val rmse 0.2421\n",
      "train mse 0.0770 val rmse 0.2428\n"
     ]
    }
   ],
   "source": [
    "train_model_regr(model, epochs=20, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# Read the test excerpts\n",
    "test_data = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "print(test_data.head())\n",
    "\n",
    "# Apply the same encoding as the train texts\n",
    "test_data['encoded'] = test_data['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "idx, excerpts_test = test_data['id'], test_data['encoded']\n",
    "\n",
    "X_test = [excerpts_test[i][0] for i in range(len(test_data))]\n",
    "l_test = [excerpts_test[i][1] for i in range(len(test_data))]\n",
    "X_test = torch.LongTensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115de37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
